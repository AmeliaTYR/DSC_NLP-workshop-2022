{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP for NUS restaurant reviews(Participant).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmeliaTYR/DSC_NLP-workshop-2022/blob/main/NLP_for_NUS_restaurant_reviews(Participant).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwK5-9FIB-lu"
      },
      "source": [
        "# Natural Language Processing applied to NUS restaurant reviews!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1kiO9kACE6s"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QG7sxmoCIvN"
      },
      "source": [
        "import numpy as np\n",
        "#for visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "#python based data analytics toolkit\n",
        "import pandas as pd\n",
        "#this is simply for coloring the confusion matrix. Green for successful prediction, red for failed prediction\n",
        "import sys\n",
        "from termcolor import colored, cprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTfaCIzdCLPA"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCK6vQ5QCQJe"
      },
      "source": [
        "dataset = pd.read_csv('NUSRestaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6tXTs5ntOxO"
      },
      "source": [
        "- tsv files contain elements seperated by tabs and csv files contain elements seperated by commas which can interfere with the commas within the text so we use tsv files for NLP\n",
        "- delimiter argument is used to specify to the read_csv method that this file is in tsv format\n",
        "- quoting = 3 is needed to ensure that the use of quotes in the text are not misinterpreted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qekztq71CixT"
      },
      "source": [
        "## Cleaning the texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u_yXh9dCmEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aa43d0d-2d5e-4223-f512-8064c8d595db"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "corpus = []\n",
        "# this list will contain only the 'cleaned' words\n",
        "for i in range(0, 1000):\n",
        "  review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
        "  # the 1st argument specifies what should be replaced, the 2nd one specifies the replacement term and the 3rd one specifies the location of the term\n",
        "  review = review.lower()\n",
        "  review = review.split()\n",
        "  # this splitting is done to get individual words so that the stem method can be applied to each word\n",
        "  ps = PorterStemmer()\n",
        "  all_stopwords = stopwords.words('english')\n",
        "  all_stopwords.remove('not')\n",
        "  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n",
        "  review = ' '.join(review)\n",
        "  corpus.append(review)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Oj-mo1AwIXp"
      },
      "source": [
        "Types of 'cleaning' done:\n",
        "- removal of stopwords(i.e. 'the', 'a', 'on')\n",
        "- retrieving the essence/base form of remaining words(i.e. 'love' instead of 'loved'\n",
        "- replacing of non-alphabetical features with spaces\n",
        "- uppercase to lower case  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpGWdrzGoAsL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3538a048-806e-45eb-e334-d2f117283fd7"
      },
      "source": [
        "#The key words that our model will be looking at\n",
        "print(corpus[-24:-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['good food ambienc visit dinner incred tasti authent person favourit vegetarian pizza top best freshest ingredi crust bake utmost perfect', 'ambienc wanderlust relax servic great', 'order three drink ice cream lava cake set one satchet sugar plain water need ask italian place umhm next tabl order set drink serv ice water without ask price menu', 'overal great place', 'great food ambienc not go back disappoint pizza pasta must tri', 'spaciou nice decor cafe good place chill friend famili price food reason well worth money paid ox tail stew highlight seabass pizza also tast good', 'seem minor chang decor sinc last visit almost year ago overal still offer function cosi enough set food menu seem expand letdown servic staff not attent diner wave attent good dine experi overal afternot chicken wing still good mani year dine', 'pasta tast realli aw', 'bimbabap terribl bowl not sizzl hotston type food luke warm best tasteless not want say anymor unagi rice averag satay best food tabl servic staff good quick realli busi saturday nite not enjoy meal perhap not order hous favourit', 'super nice', 'food great', 'bomb realli super nice sambal chilli chilli cucumb super nice spici add challeng fell love sinc', 'rare write bad review maggi goreng absolut worst ever first fri dish presum fri could not opt fri fri unapp noodl wors dri bland noth resembl maggi goreng spare tastebud go authent indian place store western indian whatev fusion thing absolut not work not return', 'awesom supper place chill air condit great varieti food sadli prata', 'read review concept small plate ridicul tast even worst disappoint price pay', 'ridicul overpr stay away oyster partial frozen cancel rest order chanc rip', 'would give star possibl horribl place', 'rude restaur waitress manag late ladi deni order mango sticki rice ask us get order dish', 'good sead food spici food overlook river nice night life friend famili', 'realli tasti pasta realli worth price highli recommend', 'super bang buck', 'ok lah choos pasta sauc get top', 'goto place pasta afford consist comfort']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLqmAkANCp1-"
      },
      "source": [
        "## Creating the Bag of Words model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qroF7XcSCvY3"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features = 1500)\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH_VjgPzC2cd"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQXYM5VzDDDI"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#Test size = 0.024 -> the 24 NUS restaurant reviews that we have as our training set\n",
        "#The remaining 976 entries were collected online to train our model on restaurant reviews\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.024, random_state = 0, shuffle = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkIq23vEDIPt"
      },
      "source": [
        "## Training the Naive Bayes model on the Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS9oiDXXDRdI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91992554-edf4-4c82-f156-8c2d6068de02"
      },
      "source": [
        "#Naive Bayes assumes that each input variable is independent\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB()"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JaRM7zXDWUy"
      },
      "source": [
        "## Predicting the Test set results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iif0CVhFDaMp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22f9adf2-4cdd-4e80-f308-ee5cb8a5d5b0"
      },
      "source": [
        "y_pred = classifier.predict(X_test)\n",
        "text = np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)\n",
        "for item in text:\n",
        "  if item[0] != item[1]:\n",
        "    print(colored(item, 'red', attrs=['reverse','blink']))\n",
        "  else:\n",
        "    print(colored(item, 'green', attrs=['reverse', 'blink']))\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[5m\u001b[7m\u001b[32m[1 1]\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m[1 1]\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m[0 0]\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m[1 1]\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m[1 1]\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m[1 1]\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[31m[0 1]\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m[0 0]\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m[0 0]\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m[1 1]\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m[1 1]\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m[1 1]\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m[0 0]\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m[1 1]\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m[0 0]\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m[0 0]\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m[0 0]\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m[0 0]\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m[1 1]\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m[1 1]\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m[1 1]\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m[1 1]\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m[1 1]\u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[32m[1 1]\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoMltea5Dir1"
      },
      "source": [
        "## Making the Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj9IU6MxDnvo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8015bf2-19c6-4c9e-d08b-576d62b1d432"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 8  0]\n",
            " [ 1 15]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9583333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}